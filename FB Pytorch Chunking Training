{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FB Pytorch Chunking Training","provenance":[],"collapsed_sections":[],"background_execution":"on","mount_file_id":"1LgtcjW9HACecg2xVkfiBc0wiF7M7qdGe","authorship_tag":"ABX9TyN2C2TWARB51mG1IUpGofrY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e2786a6a3313422d9c51a5adeefce553":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_44c28925c54047aaa26699355c397c90","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2c91d9e5b6a24635aa179bc339f40919","IPY_MODEL_597503c974b24a849144f8e4000ab6ee","IPY_MODEL_2038208368864c2aa0d8d4cca71c8081"]}},"44c28925c54047aaa26699355c397c90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c91d9e5b6a24635aa179bc339f40919":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4664f72cbc334641a485b3b7de6bf6f8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  6%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7b28343c741346d797e1f570065777ed"}},"597503c974b24a849144f8e4000ab6ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cb203ba859ed46a4ba3fa2cbdaa906b2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":4462,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a9db6a6117394fdd874f32121277b04e"}},"2038208368864c2aa0d8d4cca71c8081":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_82558c70a8e14b0082c71fb0998706cb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 267/4462 [00:46&lt;11:38,  6.01it/s, LR=2.5e-6, Train_F1=0.042, Train_Loss=2.26]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fe8e10e5013a412fafd8d1702bbc0c25"}},"4664f72cbc334641a485b3b7de6bf6f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7b28343c741346d797e1f570065777ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb203ba859ed46a4ba3fa2cbdaa906b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a9db6a6117394fdd874f32121277b04e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82558c70a8e14b0082c71fb0998706cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fe8e10e5013a412fafd8d1702bbc0c25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atZ0sQul-Msn","executionInfo":{"status":"ok","timestamp":1646436708443,"user_tz":-330,"elapsed":23,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}},"outputId":"2dfcdf4c-2524-4b90-ba23-9c397fdf55e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Mar  4 23:31:47 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["# ! gdown --id 1i2x7osfloYkXDHMRn0WuVs2P3m751cj1\n","# ! pip -q uninstall -y kaggle\n","# ! pip -q install --upgrade pip\n","# ! pip -q install kaggle --upgrade\n","# ! mkdir ~/.kaggle\n","# ! cp kaggle.json ~/.kaggle/\n","# ! chmod 600 ~/.kaggle/kaggle.json\n","# ! kaggle competitions download feedback-prize-2021\n","# ! kaggle datasets download aishikai/fb-corrected-train\n","# ! kaggle datasets download nbroad/deberta-v2-3-fast-tokenizer"],"metadata":{"id":"mxnJnnqnGkq2","executionInfo":{"status":"ok","timestamp":1646436708444,"user_tz":-330,"elapsed":19,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ! unzip -q /content/feedback-prize-2021.zip -d data\n","# ! rm /content/feedback-prize-2021.zip\n","# ! unzip -q /content/fb-corrected-train.zip -d data\n","# ! rm /content/fb-corrected-train.zip\n","# ! unzip /content/deberta-v2-3-fast-tokenizer.zip -d tokenizer\n","# ! rm /content/deberta-v2-3-fast-tokenizer.zip"],"metadata":{"id":"pGHzc0a4GlSo","executionInfo":{"status":"ok","timestamp":1646436708444,"user_tz":-330,"elapsed":18,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# !pip install -qq sentencepiece\n","# !pip install -qq transformers\n","# !pip install -qq iterative-stratification"],"metadata":{"id":"raT6nB9KGmdc","executionInfo":{"status":"ok","timestamp":1646436708445,"user_tz":-330,"elapsed":19,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import shutil\n","from pathlib import Path\n","\n","transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","\n","input_dir = Path(\"/content/tokenizer\")\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)"],"metadata":{"id":"fqH0yt_SXopz","executionInfo":{"status":"ok","timestamp":1646436708445,"user_tz":-330,"elapsed":18,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# general\n","import pandas as pd\n","import numpy as np\n","import os\n","import ast\n","import copy\n","import random\n","from joblib import Parallel, delayed\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics\n","from sklearn.model_selection import KFold\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","import gc\n","from collections import defaultdict\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","# nlp\n","from sklearn.feature_extraction.text import CountVectorizer\n","import torch\n","import torch.nn as nn\n","from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_cosine_schedule_with_warmup\n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler"],"metadata":{"id":"gxYu9EeHGooS","executionInfo":{"status":"ok","timestamp":1646436713192,"user_tz":-330,"elapsed":4764,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Config:\n","    savename = \"deberta-base\"\n","    seed = 2022\n","    n_folds = 5\n","    num_workers = 2\n","    fold = 0\n","    model = \"microsoft/deberta-base\"\n","    lr = 2.5e-5\n","    n_accum = 1\n","    output = \"/content/model\"\n","    input = \"/content/data/\"\n","    ner_csv = \"/content/train_NER.csv\"\n","    max_len = 512\n","    stride = 128\n","    num_labels = 15\n","    batch_size = 4\n","    valid_batch_size = 4\n","    epochs = 6\n","    accumulation_steps = 1\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    apex = True\n","    debug = False\n","    if debug:\n","        n_folds = 2\n","        epochs = 2"],"metadata":{"id":"5TCwp13AGqbR","executionInfo":{"status":"ok","timestamp":1646436713193,"user_tz":-330,"elapsed":82,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed=42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","set_seed(Config.seed)"],"metadata":{"id":"CGJ9eRzx4CYL","executionInfo":{"status":"ok","timestamp":1646436713194,"user_tz":-330,"elapsed":81,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def create_folds(df):\n","    dfx = pd.get_dummies(df, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\n","    cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\n","    dfx = dfx[cols]\n","\n","    mskf = MultilabelStratifiedKFold(n_splits=Config.n_folds, shuffle=True, random_state=42)\n","    labels = [c for c in dfx.columns if c != \"id\"]\n","    dfx_labels = dfx[labels]\n","    dfx[\"kfold\"] = -1\n","\n","    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\n","        print(len(trn_), len(val_))\n","        dfx.loc[val_, \"kfold\"] = fold\n","\n","    df = df.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\n","    print(df.kfold.value_counts())\n","    df.to_csv(\"train_folds.csv\", index=False)\n","    return df"],"metadata":{"id":"4YfiwLnIIV6d","executionInfo":{"status":"ok","timestamp":1646436713194,"user_tz":-330,"elapsed":79,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def assign_labels(df_all):\n","    if os.path.isfile(Config.ner_csv):\n","        df_texts = pd.read_csv(Config.ner_csv,\n","                            converters={'entities':ast.literal_eval, 'text_split': ast.literal_eval})\n","    else:\n","        train_names, train_texts = [], []\n","        for f in tqdm(list(os.listdir(os.path.join(Config.input, \"train\")))):\n","            train_names.append(f.replace('.txt', ''))\n","            train_texts.append(open(os.path.join(Config.input, \"train\") + \"/\" + f, 'r').read())\n","\n","            df_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\n","\n","        df_texts['text_split'] = df_texts.text.str.split()\n","        \n","        all_entities = []\n","        for _, row in tqdm(df_texts.iterrows(), total=len(df_texts)):\n","\n","            total = len(row['text_split'])\n","            entities = [\"O\"] * total\n","\n","            for _, row2 in df_all[df_all['id'] == row['id']].iterrows():\n","                discourse = row2['discourse_type']\n","                list_ix = [int(x) for x in row2['predictionstring'].split(' ')]\n","                entities[list_ix[0]] = f\"B-{discourse}\"\n","                for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n","            all_entities.append(entities)\n","\n","        df_texts['entities'] = all_entities\n","        df_texts.to_csv(Config.ner_csv ,index=False)\n","        \n","    print(df_texts.shape)\n","    return df_texts"],"metadata":{"id":"zoQx8DdNG9ug","executionInfo":{"status":"ok","timestamp":1646436713194,"user_tz":-330,"elapsed":79,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n","          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n","\n","LABELS_TO_IDS = {v:k for k,v in enumerate(output_labels)}\n","LABELS_TO_IDS[\"PAD\"] = -100\n","IDS_TO_LABELS = {k:v for k,v in enumerate(output_labels)}\n","IDS_TO_LABELS[-100] = \"PAD\""],"metadata":{"id":"m2_cqsXJIBZ-","executionInfo":{"status":"ok","timestamp":1646436713195,"user_tz":-330,"elapsed":79,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_labels(word_ids, word_labels):\n","    label_ids = []\n","    for word_idx in word_ids:                            \n","        if word_idx is None:\n","            label_ids.append(-100)\n","        else:\n","            label_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n","    return label_ids\n","\n","# Tokenize texts, possibly generating more than one tokenized sample for each text\n","def tokenize(df, to_tensor=True, with_labels=True):\n","    # This is what's different from a longformer\n","    # Read the parameters with attention\n","    encoded = tokenizer(df['text_split'].tolist(),\n","                        is_split_into_words=True,\n","                        return_overflowing_tokens=True,\n","                        stride=Config.stride,\n","                        max_length=Config.max_len,\n","                        padding=\"max_length\",\n","                        truncation=True)\n","\n","    if with_labels:\n","        encoded['labels'] = []\n","\n","    encoded['wids'] = []\n","    n = len(encoded['overflow_to_sample_mapping'])\n","    for i in range(n):\n","\n","        # Map back to original row\n","        text_idx = encoded['overflow_to_sample_mapping'][i]\n","        \n","        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n","        word_ids = encoded.word_ids(i)\n","        \n","        if with_labels:\n","            # Get word labels of the full un-chunked text\n","            word_labels = df['entities'].iloc[text_idx]\n","        \n","            # Get the labels associated with the word indexes\n","            label_ids = get_labels(word_ids, word_labels)\n","            encoded['labels'].append(label_ids)\n","        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n","    if to_tensor:\n","        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n","    return encoded"],"metadata":{"id":"hoHzcVz8OHMS","executionInfo":{"status":"ok","timestamp":1646436713196,"user_tz":-330,"elapsed":79,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class FeedbackPrizeDataset(Dataset):\n","    def __init__(self, tokenized_ds):\n","        self.data = tokenized_ds\n","\n","    def __getitem__(self, index):\n","        item = {k: self.data[k][index] for k in self.data.keys()}\n","        return item\n","\n","    def __len__(self):\n","        return len(self.data['input_ids'])"],"metadata":{"id":"r2Oe4UQXOQDN","executionInfo":{"status":"ok","timestamp":1646436713196,"user_tz":-330,"elapsed":78,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class FeedbackModel(nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model_name = model_name\n","        self.num_labels = num_labels\n","\n","        hidden_dropout_prob: float = 0.1\n","        layer_norm_eps: float = 1e-7\n","\n","        config = AutoConfig.from_pretrained(model_name)\n","\n","        config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": hidden_dropout_prob,\n","                \"layer_norm_eps\": layer_norm_eps,\n","                \"add_pooling_layer\": False,\n","                \"num_labels\": self.num_labels,\n","            }\n","        )\n","        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.dropout1 = nn.Dropout(0.1)\n","        self.dropout2 = nn.Dropout(0.2)\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.dropout4 = nn.Dropout(0.4)\n","        self.dropout5 = nn.Dropout(0.5)\n","        self.output = nn.Linear(config.hidden_size, self.num_labels)\n","\n","    def forward(self, ids, mask, token_type_ids=None):\n","\n","        if token_type_ids:\n","            transformer_out = self.transformer(ids, mask, token_type_ids)\n","        else:\n","            transformer_out = self.transformer(ids, mask)\n","        sequence_output = transformer_out.last_hidden_state\n","        sequence_output = self.dropout(sequence_output)\n","\n","        logits1 = self.output(self.dropout1(sequence_output))\n","        logits2 = self.output(self.dropout2(sequence_output))\n","        logits3 = self.output(self.dropout3(sequence_output))\n","        logits4 = self.output(self.dropout4(sequence_output))\n","        logits5 = self.output(self.dropout5(sequence_output))\n","\n","        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n","        return logits, logits1, logits2, logits3, logits4, logits5"],"metadata":{"id":"RwvZOIveITAn","executionInfo":{"status":"ok","timestamp":1646436713197,"user_tz":-330,"elapsed":78,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def criterion (outputs, targets, attention_mask):\n","    loss_fct = nn.CrossEntropyLoss()\n","\n","    active_loss = attention_mask.view(-1) == 1\n","    active_logits = outputs.view(-1, Config.num_labels)\n","    true_labels = targets.view(-1)\n","    outputs = active_logits.argmax(dim=-1)\n","    idxs = np.where(active_loss.cpu().numpy() == 1)[0]\n","    active_logits = active_logits[idxs]\n","    true_labels = true_labels[idxs].to(torch.long)\n","\n","    loss = loss_fct(active_logits, true_labels)\n","    return loss"],"metadata":{"id":"ZEE9raxAPTwR","executionInfo":{"status":"ok","timestamp":1646436713197,"user_tz":-330,"elapsed":77,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def fetch_optimizer(model, learning_rate = Config.lr):\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\"]\n","    optimizer_parameters = [\n","        {\n","            \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.01,\n","        },\n","        {\n","            \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    opt = AdamW(optimizer_parameters, lr=learning_rate)\n","    return opt\n","\n","def fetch_scheduler(optimizer, num_train_steps):\n","    sch = get_cosine_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=int(0.1 *  num_train_steps),\n","        num_training_steps= num_train_steps,\n","        num_cycles=1,\n","        last_epoch=-1,\n","    )\n","    return sch"],"metadata":{"id":"i0D1gxtZKrev","executionInfo":{"status":"ok","timestamp":1646436713197,"user_tz":-330,"elapsed":77,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def monitor_metrics(outputs, targets, attention_mask):\n","    active_loss = (attention_mask.view(-1) == 1).cpu().numpy()\n","    active_logits = outputs.view(-1, Config.num_labels)\n","    true_labels = targets.view(-1).cpu().numpy()\n","    outputs = active_logits.argmax(dim=-1).cpu().numpy()\n","    idxs = np.where(active_loss == 1)[0]\n","    f1_score = metrics.f1_score(true_labels[idxs], outputs[idxs], average=\"macro\")\n","    return f1_score"],"metadata":{"id":"wYuCSAcFOFJ7","executionInfo":{"status":"ok","timestamp":1646436713197,"user_tz":-330,"elapsed":77,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["## Metric Utils\n","def calc_overlap(row):\n","    \"\"\"\n","    Calculates the overlap between prediction and\n","    ground truth and overlap percentages used for determining\n","    true positives.\n","    \"\"\"\n","    set_pred = set(row.predictionstring_pred.split(\" \"))\n","    set_gt = set(row.predictionstring_gt.split(\" \"))\n","    # Length of each and intersection\n","    len_gt = len(set_gt)\n","    len_pred = len(set_pred)\n","    inter = len(set_gt.intersection(set_pred))\n","    overlap_1 = inter / len_gt\n","    overlap_2 = inter / len_pred\n","    return [overlap_1, overlap_2]\n","\n","\n","def score_feedback_comp(pred_df, gt_df):\n","    \"\"\"\n","    A function that scores for the kaggle\n","        Student Writing Competition\n","        \n","    Uses the steps in the evaluation page here:\n","        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n","    \"\"\"\n","    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n","        .reset_index(drop=True).copy()\n","    pred_df = pred_df[['id','class','predictionstring']] \\\n","        .reset_index(drop=True).copy()\n","    pred_df['pred_id'] = pred_df.index\n","    gt_df['gt_id'] = gt_df.index\n","    # Step 1. all ground truths and predictions for a given class are compared.\n","    joined = pred_df.merge(gt_df,\n","                           left_on=['id','class'],\n","                           right_on=['id','discourse_type'],\n","                           how='outer',\n","                           suffixes=('_pred','_gt')\n","                          )\n","    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n","    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n","\n","    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n","\n","    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n","    # and the overlap between the prediction and the ground truth >= 0.5,\n","    # the prediction is a match and considered a true positive.\n","    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n","    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n","    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n","\n","\n","    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n","    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n","    tp_pred_ids = joined.query('potential_TP') \\\n","        .sort_values('max_overlap', ascending=False) \\\n","        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n","\n","    # 3. Any unmatched ground truths are false negatives\n","    # and any unmatched predictions are false positives.\n","    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n","\n","    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n","    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n","\n","    # Get numbers of each type\n","    TP = len(tp_pred_ids)\n","    FP = len(fp_pred_ids)\n","    FN = len(unmatched_gt_ids)\n","    #calc microf1\n","    my_f1_score = TP / (TP + 0.5*(FP+FN))\n","    return my_f1_score"],"metadata":{"id":"ms-5uRynRhz2","executionInfo":{"status":"ok","timestamp":1646436713197,"user_tz":-330,"elapsed":76,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def get_predictions(all_labels, all_scores, df):    \n","    proba_thresh = {\n","        \"Lead\": 0.7,\n","        \"Position\": 0.55,\n","        \"Evidence\": 0.65,\n","        \"Claim\": 0.55,\n","        \"Concluding Statement\": 0.7,\n","        \"Counterclaim\": 0.5,\n","        \"Rebuttal\": 0.55,\n","    }\n","    final_preds = []\n","    \n","    for i in range(len(df)):\n","        idx = df.id.values[i]\n","        pred = all_labels[i]\n","        score = all_scores[i]\n","        preds = []\n","        j = 0\n","        \n","        while j < len(pred):\n","            cls = pred[j]\n","            if cls == 'O': pass\n","            else: cls = cls.replace('B','I')\n","            end = j + 1\n","            while end < len(pred) and pred[end] == cls:\n","                end += 1\n","            if cls != 'O' and cls != '' and end - j > 7:\n","                if np.mean(score[j:end]) > proba_thresh[cls.replace('I-','')]:\n","                    final_preds.append((idx, cls.replace('I-',''), \n","                                        ' '.join(map(str, list(range(j, end))))))\n","            j = end\n","    df_pred = pd.DataFrame(final_preds)\n","    df_pred.columns = ['id','class','predictionstring']\n","    return df_pred\n","\n","def threshold(df):\n","\n","    min_thresh = {\n","        \"Lead\": 9,\n","        \"Position\": 5,\n","        \"Evidence\": 14,\n","        \"Claim\": 3,\n","        \"Concluding Statement\": 11,\n","        \"Counterclaim\": 6,\n","        \"Rebuttal\": 4,\n","    }\n","\n","    df = df.copy()\n","    for key, value in min_thresh.items():\n","        index = df.loc[df[\"class\"] == key].query(f\"len<{value}\").index\n","        df.drop(index, inplace=True)\n","    return df\n","\n","def compute_val_f1(oof, df_valid):\n","    oof[\"len\"] = oof.predictionstring.apply(lambda x: len(x.split()))\n","    oof = threshold(oof)\n","    # Compute F1-score\n","    f1s = []\n","    classes = oof['class'].unique()\n","    \n","    f1s_log = {}\n","    for c in classes:\n","        pred_df = oof.loc[oof['class']==c].copy()\n","        gt_df = df_valid.loc[df_valid['discourse_type']==c].copy()\n","        f1 = score_feedback_comp(pred_df, gt_df)\n","        f1s.append(f1)\n","        f1s_log[f'F1 {c}'] = f1\n","    \n","    f1s_log['Overall F1'] = np.mean(f1s)\n","    return f1s_log"],"metadata":{"id":"CYF3u6WPWhQV","executionInfo":{"status":"ok","timestamp":1646436713198,"user_tz":-330,"elapsed":76,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","    def __init__(self, config, dataloaders, valid_df, valid_df_, optimizer, model, loss_fns, scheduler, device=Config.device, apex=Config.apex):\n","        self.train_loader, self.valid_loader = dataloaders\n","        self.train_loss_fn = loss_fns\n","        self.valid_df = valid_df\n","        self.valid_df_ = valid_df_\n","        self.scheduler = scheduler\n","        self.optimizer = optimizer\n","        self.model = model\n","        self.device = device\n","        self.apex = apex\n","        self.Config = config\n","    \n","    def train_one_epoch(self):\n","        scaler = GradScaler()\n","\n","        self.model.train()\n","        train_pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n","        dataset_size = 0\n","        running_loss = 0.0\n","        running_f1 = 0.0\n","\n","        for step, data in train_pbar:        \n","            input_ids = data[\"input_ids\"].to(self.device)\n","            input_mask = data[\"attention_mask\"].to(self.device)\n","            targets = data[\"labels\"].to(self.device)\n","\n","            batch_size = input_ids.shape[0]\n","\n","            with(autocast(enabled = True)):\n","                logits, logits1, logits2, logits3, logits4, logits5 = self.model(input_ids,\n","                                                                                input_mask)\n","                \n","                loss1 = self.train_loss_fn(logits1, targets, input_mask)\n","                loss2 = self.train_loss_fn(logits2, targets, input_mask)\n","                loss3 = self.train_loss_fn(logits3, targets, input_mask)\n","                loss4 = self.train_loss_fn(logits4, targets, input_mask)\n","                loss5 = self.train_loss_fn(logits5, targets, input_mask)\n","                loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5.0\n","                loss /= self.Config.n_accum\n","\n","            scaler.scale(loss).backward()\n","\n","            if (step + 1) % self.Config.n_accum == 0:\n","                    scaler.step(self.optimizer)\n","                    scaler.update()\n","                    self.optimizer.zero_grad()\n","                    self.scheduler.step()\n","\n","            f1_1 = monitor_metrics(logits1, targets, input_mask)\n","            f1_2 = monitor_metrics(logits2, targets, input_mask)\n","            f1_3 = monitor_metrics(logits3, targets, input_mask)\n","            f1_4 = monitor_metrics(logits4, targets, input_mask)\n","            f1_5 = monitor_metrics(logits5, targets, input_mask)\n","            f1 = (f1_1 + f1_2 + f1_3 + f1_4 + f1_5) / 5.0\n","\n","            running_loss += (loss.item() * batch_size)  \n","            running_f1 += (f1 * batch_size)  \n","            dataset_size += batch_size\n","            epoch_loss = running_loss / dataset_size\n","            epoch_f1 = running_f1 / dataset_size\n","\n","            train_pbar.set_postfix(Train_Loss = epoch_loss, Train_F1 = epoch_f1, LR = self.optimizer.param_groups[0]['lr'])\n","\n","        return epoch_loss, epoch_f1\n","\n","    @torch.no_grad()\n","    def valid_one_epoch(self):\n","\n","        self.model.eval()\n","        valid_pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader))\n","        dataset_size = 0\n","        valid_preds = []\n","        final_preds = []\n","        final_scores = []\n","        running_loss = 0.0\n","        predictions = defaultdict(list)\n","        prediction_scores = defaultdict(list)\n","        seen_words_idx = defaultdict(list)\n","\n","        for step , data in valid_pbar:\n","            input_ids = data[\"input_ids\"].to(self.device)\n","            input_mask = data[\"attention_mask\"].to(self.device)\n","\n","            batch_size = input_ids.shape[0]\n","            logits, logits1, logits2, logits3, logits4, logits5 = self.model(input_ids,\n","                                                                            input_mask)\n","            \n","            val_preds = logits.cpu().tolist()\n","            batch_preds = np.argmax(val_preds, axis=-1)\n","            batch_scores = np.max(val_preds, axis=-1)\n","            \n","            for k, (chunk_preds, chunk_scores, text_id) in enumerate(zip(batch_preds, batch_scores, data[\"overflow_to_sample_mapping\"].tolist())):\n","                # The word_ids are absolute references in the original text\n","                word_ids = data['wids'][k].numpy()\n","                \n","                # Map from ids to labels\n","                chunk_preds = [IDS_TO_LABELS[i] for i in chunk_preds]        \n","                \n","                for idx, word_idx in enumerate(word_ids):                            \n","                    if word_idx == -1:\n","                        pass\n","                    elif word_idx not in seen_words_idx[text_id]:\n","                        # Add predictions if the word doesn't have a prediction from a previous chunk\n","                        predictions[text_id].append(chunk_preds[idx])\n","                        prediction_scores[text_id].append(chunk_scores[idx])\n","                        seen_words_idx[text_id].append(word_idx)\n","            \n","        final_predictions = [predictions[k] for k in sorted(predictions.keys())]\n","        final_scores = [prediction_scores[k] for k in sorted(prediction_scores.keys())]\n","\n","        df_pred = get_predictions(final_predictions, final_scores, self.valid_df_)\n","        f1s_log = compute_val_f1(df_pred, self.valid_df)\n","\n","        return f1s_log\n","            \n","\n","    def fit(self, fold: str, epochs: int = 10, output_dir: str = \"/content/models/\", custom_name: str = 'model.pth'):\n","        \"\"\"\n","        Low-effort alternative for doing the complete training and validation process\n","        \"\"\"\n","        best_score = int(-1e+7)\n","        for epx in range(epochs):\n","            print(f\"{'='*20} Epoch: {epx+1} / {epochs} {'='*20}\")\n","\n","            train_loss, train_f1 = self.train_one_epoch()\n","            print(f\"Training loss: {train_loss:.4f} Training f1: {train_f1:.4f}\")\n","\n","            valid_score = self.valid_one_epoch()\n","\n","            print(valid_score)\n","            \n","            valid_score = valid_score['Overall F1']\n","            print(f'Validation Score: {valid_score:.4f}')\n","\n","            custom_name = f\"model_{fold}\"\n","            if valid_score > best_score:\n","                best_score = valid_score\n","                self.save_model(output_dir, custom_name)\n","                print(f\"Saved model with val_score: {best_score:.4f}\")\n","            \n","\n","    def save_model(self, path, name, verbose=False):\n","        \"\"\"\n","        Saves the model at the provided destination\n","        \"\"\"\n","        try:\n","            if not os.path.exists(path):\n","                os.makedirs(path)\n","        except:\n","            print(\"Errors encountered while making the output directory\")\n","\n","        torch.save(self.model.state_dict(), os.path.join(path, name))\n","        if verbose:\n","            print(f\"Model Saved at: {os.path.join(path, name)}\")"],"metadata":{"id":"2ahgWkbVRi_N","executionInfo":{"status":"ok","timestamp":1646436714489,"user_tz":-330,"elapsed":1366,"user":{"displayName":"Aishik Rakshit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCKXVUpojDkD0Qb-JKIFieuoaxGhayWRIW_RNLKg=s64","userId":"15122983906308089605"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    df = pd.read_csv(os.path.join(Config.input, \"corrected_train.csv\"))\n","    if Config.debug:\n","        df = df[:100]\n","    df = create_folds(df)\n","    df_text = assign_labels(df)\n","\n","    os.makedirs(\"models\", exist_ok = True)\n","    for fold in range(Config.n_folds):\n","\n","        print(f\"Training Fold {fold}\")\n","        train_df = df[df[\"kfold\"] != fold].reset_index(drop=True)\n","        valid_df = df[df[\"kfold\"] == fold].reset_index(drop=True)\n","        train_idx = train_df[\"id\"].unique()\n","        valid_idx = valid_df[\"id\"].unique()\n","        train_df_ = df_text.loc[df_text.id.isin(train_idx)]\n","        valid_df_ = df_text.loc[df_text.id.isin(valid_idx)]\n","        tokenizer = AutoTokenizer.from_pretrained(Config.model, add_prefix_space=True)\n","\n","        tokenized_train = tokenize(train_df_)\n","        tokenized_val = tokenize(valid_df_)\n","\n","        train_dataset = FeedbackPrizeDataset(tokenized_train)\n","        valid_dataset = FeedbackPrizeDataset(tokenized_val)\n","\n","        num_train_steps = int(len(train_dataset) / Config.batch_size / Config.accumulation_steps * Config.epochs)\n","        model = FeedbackModel(Config.model,\n","                               Config.num_labels)        \n","        model.to(Config.device)\n","        optimizer = fetch_optimizer(model)\n","        scheduler = fetch_scheduler(optimizer, num_train_steps)\n","        \n","        train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                                batch_size = Config.batch_size,\n","                                                pin_memory = True,\n","                                                num_workers = Config.num_workers,  \n","                                                shuffle = True)\n","        \n","        valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","                                                batch_size = Config.valid_batch_size,\n","                                                pin_memory = True,\n","                                                num_workers = Config.num_workers,  \n","                                                shuffle = False)\n","        \n","        trainer = Trainer(config = Config,\n","                        dataloaders =(train_loader, valid_loader) ,\n","                        valid_df = valid_df,\n","                        valid_df_ = valid_df_,\n","                        optimizer = optimizer,\n","                        model = model,\n","                        loss_fns = criterion,\n","                        scheduler = scheduler)\n","        \n","        trainer.fit(fold,\n","                    Config.epochs)\n","        \n","        del model\n","        gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e2786a6a3313422d9c51a5adeefce553","44c28925c54047aaa26699355c397c90","2c91d9e5b6a24635aa179bc339f40919","597503c974b24a849144f8e4000ab6ee","2038208368864c2aa0d8d4cca71c8081","4664f72cbc334641a485b3b7de6bf6f8","7b28343c741346d797e1f570065777ed","cb203ba859ed46a4ba3fa2cbdaa906b2","a9db6a6117394fdd874f32121277b04e","82558c70a8e14b0082c71fb0998706cb","fe8e10e5013a412fafd8d1702bbc0c25"],"output_embedded_package_id":"18iztjWVaakUYPu69zi-ZBnrbk7UXJNxz"},"id":"2h7ffLCbbjnw","outputId":"9d037ae9-c615-4392-d5e0-701700fe44b2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["savefile = f\"/content/drive/MyDrive/FB_{Config.savename}_chunks.zip\"\n","! zip -r $savefile models"],"metadata":{"id":"Yr7Dtx-k3e7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -r models"],"metadata":{"id":"gW3sivKbcPLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YsBKL_z_I1yO"},"execution_count":null,"outputs":[]}]}